# -*- coding: utf-8 -*-
"""Web Scraping (SOLVED).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HSjfM0Lo0R4uvyzobPKft4AmSFG2XdM3

# **Web Scraping**
- Website to scrape data: http://books.toscrape.com/

- Importing libraries
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd

"""- Define the URL of the website you want to scrape"""

url = 'http://books.toscrape.com/'

"""- Use the requests.get() function to send an HTTP GET request to the specified URL and store the response."""

response = requests.get(url)
response

"""- The line response.status_code is used to check the HTTP status code returned by the web server in response to the HTTP request"""

response.status_code

"""- The line type(response) is used to determine the type of the object response"""

type(response)

"""- The line print(response.text[:1000]) is printing the first 1000 characters of the text content received in the HTTP response."""

print(response.text[:1000])

"""- The type(response.text) is used to determine the type of the text attribute of the response object"""

type(response.text)

"""- BeautifulSoup(response.text): This creates a BeautifulSoup object (soup) by parsing the HTML content of the response.text. response.text is the text content of the HTTP response received from the server.

- type(soup): This line checks and prints the type of the soup object. The BeautifulSoup function returns an object of the bs4.BeautifulSoup class.
"""

soup = BeautifulSoup(response.text)
type(soup)

"""- The line books_tag = soup.find_all('article', class_='product_pod') is using BeautifulSoup to find all HTML elements with the tag 'a' and the class attribute set to 'product_pod' within the BeautifulSoup object (soup)."""

books_tag = soup.find_all('article',class_='product_pod')

"""- len(books_tag) gives you the number of elements in the list books_tag"""

len(books_tag)

"""- Used to get the 20th book from the list books_tag"""

book = books_tag[19]
print(book)

"""- Used to extract the title of a book from the HTML structure of the selected book"""

title_tag = book.find('a',title=True)['title']
title_tag

"""- Used to extract the rating information of a book from the HTML structure of the selected book"""

rating_tag = book.find('p')['class'][1]
rating_tag

"""- Used to extract the price of a book from the HTML structure of the selected book"""

price_tag = book.find('p',class_='price_color').text[1:]
price_tag

"""- Used to construct the complete URL link for a book based on the information found in the HTML structure of the selected book"""

link_tag = 'http://books.toscrape.com/' + book.find('a')['href']
link_tag

"""- The code is part of a loop that iterates over the first 20 books in the 'books_tag' list, extracts various information for each book, and appends it to the 'data_list'."""

data_list = []

for books in range(0, 20):
  book = books_tag[books]
  title_tag = book.find('a',title=True)['title']
  rating_tag = book.find('p')['class'][1]
  price_tag = book.find('p',class_='price_color').text[1:]
  link_tag = 'http://books.toscrape.com/' + book.find('a')['href']

  data_list.append([title_tag, price_tag, rating_tag, link_tag])

"""- The code segment involves creating a Pandas DataFrame (df) from the data_list"""

columns = ['Title', 'Price', 'Rating', 'Link']
df = pd.DataFrame(data_list, columns=columns)

"""- Printing the DataFrame"""

df

"""- The variable base_url contains a string representing the base URL for a series of pages on a website.
- The URL is structured with a placeholder {} where the page number should be inserted.
"""

base_url = 'http://books.toscrape.com/catalogue/page-{}.html'

"""- Number of pages to scrape"""

num_pages = 5

"""- Creating a list to store the data after being scraped"""

data_list = []

"""## **Steps to follow to scrape data from 5 pages**
- Loop through pages
- Construct URL for each page
- Make an HTTP request and create BeautifulSoup object
- Find all book articles on the page
- Extract data for each book on the page
- Append the data to the list
"""

# Loop through pages
for page_number in range(1, num_pages + 1):

  # Constructing URL for each page
  url = base_url.format(page_number)

  # Make an HTTP request and create BeautifulSoup object
  response = requests.get(url)
  soup = BeautifulSoup(response.text, 'html.parser')

  # Find all book articles on the page
  books_tag = soup.find_all('article', class_='product_pod')

  # Extract data for each book on the page
  for book in books_tag:
    title_tag = book.find('a', title=True)['title']
    rating_tag = book.find('p')['class'][1]
    price_tag = book.find('p', class_='price_color').text[1:]
    link_tag = 'http://books.toscrape.com/' + book.find('a')['href']

    # Append the data to the list
    data_list.append([title_tag, price_tag, rating_tag, link_tag])

"""- Creating a DataFrame from the list"""

columns = ['Title', 'Price', 'Rating', 'Link']
complete_df = pd.DataFrame(data_list, columns=columns)

"""Printing the DataFrame"""

complete_df

"""- Getting the shape of the DataFrame"""

complete_df.shape

"""- Converting the DataFrame to .csv file for further use"""

complete_df.to_csv('books_data.csv', index=False)